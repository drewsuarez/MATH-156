{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "3) Implement a program to train a binary logistic regression model using mini-batch SGD. Use the logistic\n",
        "regression model we derived in class, corresponding to Equation (4.90) from the textbook, and where\n",
        "the feature transformation φ is the identity function.\n",
        "The program should include the following hyperparameters:\n",
        "\n",
        "\n",
        "*   Batch size\n",
        "*   Fixed learning rate\n",
        "*   Maximum number of iterations"
      ],
      "metadata": {
        "id": "2P0m7HcHz96U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(z):\n",
        "    \"\"\"\n",
        "    Compute the sigmoid function for input z.\n",
        "\n",
        "    Parameters:\n",
        "    z (array-like): Input array or scalar.\n",
        "\n",
        "    Returns:\n",
        "    array-like: The sigmoid function applied to each element of z.\n",
        "    \"\"\"\n",
        "\n",
        "    z = np.clip(z, -500, 500)  # Clip to avoid overflow in exp\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def compute_gradients(X, y, t):\n",
        "    \"\"\"\n",
        "    Compute the gradients for the mini-batch SGD.\n",
        "\n",
        "    Parameters:\n",
        "    X (array-like): Input feature matrix with shape (n_samples, n_features).\n",
        "    y (array-like): Predicted label for each sample.\n",
        "    t (array-like): True binary labels for each sample.\n",
        "\n",
        "    Returns:\n",
        "    array-like: The gradient of the loss with respect to weights.\n",
        "    \"\"\"\n",
        "\n",
        "    m = X.shape[0]\n",
        "    error = y - t  # Difference between predicted and actual labels\n",
        "    gradients = np.dot(X.T, error) / m\n",
        "    return gradients\n",
        "\n",
        "def train_logistic_regression(X, t, learning_rate=0.01, batch_size=30, max_iters=1000):\n",
        "    \"\"\"\n",
        "    Train a logistic regression model using mini-batch stochastic gradient descent.\n",
        "\n",
        "    Parameters:\n",
        "    X (array-like): Input feature matrix with shape (n_samples, n_features).\n",
        "    t (array-like): Target binary labels (0 or 1).\n",
        "    learning_rate (float): Learning rate for weight updates.\n",
        "    batch_size (int): Number of samples per mini-batch.\n",
        "    max_iters (int): Maximum number of training iterations.\n",
        "\n",
        "    Returns:\n",
        "    array-like: Trained weight vector.\n",
        "    \"\"\"\n",
        "\n",
        "    n_samples, n_features = X.shape\n",
        "    w = np.random.normal(loc=0.0, scale=1.0, size=n_features)  # Initialize random weights from Gaussian distribution\n",
        "\n",
        "    for i in range(max_iters):\n",
        "        # Shuffle the data at each iteration\n",
        "        indices = np.random.permutation(n_samples)\n",
        "        X_shuffled = X[indices]\n",
        "        t_shuffled = t[indices]\n",
        "\n",
        "        # Mini-batch SGD\n",
        "        for start in range(0, n_samples, batch_size):\n",
        "            end = min(start + batch_size, n_samples)\n",
        "            X_batch = X_shuffled[start:end]\n",
        "            t_batch = t_shuffled[start:end]\n",
        "\n",
        "            # Compute the predicted probabilities\n",
        "            y_batch = sigmoid(np.dot(X_batch, w))\n",
        "\n",
        "            # Compute gradients and update weights\n",
        "            gradients = compute_gradients(X_batch, y_batch, t_batch)\n",
        "            w -= learning_rate * gradients\n",
        "\n",
        "    return w\n",
        "\n",
        "def predict_probability(X, w):\n",
        "    \"\"\"\n",
        "    Predict probabilities for each sample in X using trained weights.\n",
        "\n",
        "    Parameters:\n",
        "    X (array-like): Input feature matrix with shape (n_samples, n_features).\n",
        "    w (array-like): Trained weight vector.\n",
        "\n",
        "    Returns:\n",
        "    array-like: Predicted probabilities for each sample.\n",
        "    \"\"\"\n",
        "    return sigmoid(np.dot(X, w))\n",
        "\n",
        "def predict(X, w, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Predict binary class labels based on a probability threshold.\n",
        "\n",
        "    Parameters:\n",
        "    X (array-like): Input feature matrix.\n",
        "    w (array-like): Trained weight vector.\n",
        "    threshold (float): Threshold to classify as 1 if probability >= threshold.\n",
        "\n",
        "    Returns:\n",
        "    array-like: Predicted binary labels (0 or 1) for each sample.\n",
        "    \"\"\"\n",
        "\n",
        "    return (predict_probability(X, w) >= threshold).astype(int)\n",
        "\n",
        "# Testing the function\n",
        "if __name__ == \"__main__\":\n",
        "    # Generate some random binary classification data for testing\n",
        "    np.random.seed(42)\n",
        "    X = np.random.randn(1000, 3)\n",
        "    true_w = np.array([1.5, -2.0, 1.0])\n",
        "    t = (np.dot(X, true_w) + np.random.randn(1000) > 0).astype(int)\n",
        "\n",
        "    # Hyperparameters\n",
        "    learning_rate = 0.01\n",
        "    batch_size = 32\n",
        "    max_iters = 1000\n",
        "\n",
        "    # Train model\n",
        "    trained_w = train_logistic_regression(X, t, learning_rate, batch_size, max_iters)\n",
        "\n",
        "    # Predict class probabilities and labels for new data\n",
        "    X_new = np.random.randn(5, 3)\n",
        "    probabilities = predict_probability(X_new, trained_w)\n",
        "    predictions = predict(X_new, trained_w)\n",
        "\n",
        "    print(\"Predicted probabilities:\", probabilities)\n",
        "    print(\"Predicted class labels:\", predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "woOWbV23zgMy",
        "outputId": "a7b221b0-0f32-409d-b7a1-9e05f1d5a1b0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted probabilities: [0.01470187 0.43718152 0.80765623 0.43988754 0.70155581]\n",
            "Predicted class labels: [0 0 1 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. In this problem, you will run a logistic regression model for classification on a breast cancer dataset."
      ],
      "metadata": {
        "id": "_u1l0sAW0eqY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(a) Download the Wisconsin Breast Cancer dataset from the UCI Machine Learning Repository 1 or\n",
        "scikit-learn’s built-in datasets."
      ],
      "metadata": {
        "id": "tHqDpxzS0i7M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "\n",
        "# Load the breast cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Target (0: malignant, 1: benign)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhsXml321kda",
        "outputId": "4615d39d-65c1-403f-e364-5e6a29580832"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features shape: (569, 30)\n",
            "Labels shape: (569,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(b) Split the dataset into train, validation, and test sets."
      ],
      "metadata": {
        "id": "-8UwIK900vAI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the dataset into 80% train+validation and 20% test\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Further split the train+validation set into 70% train and 10% validation\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.125, random_state=42)\n",
        "\n",
        "print(f\"Training set size: {X_train.shape}\")\n",
        "print(f\"Validation set size: {X_val.shape}\")\n",
        "print(f\"Test set size: {X_test.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VC5dsFVI1-mn",
        "outputId": "e1921499-daae-4788-d7fb-4e4b7868bb37"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: (398, 30)\n",
            "Validation set size: (57, 30)\n",
            "Test set size: (114, 30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(c) Report the size of each class in your training (+ validation) set."
      ],
      "metadata": {
        "id": "yyYsGRcD3XVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine train and validation sets for class size reporting\n",
        "y_train_val_combined = np.concatenate([y_train, y_val])\n",
        "\n",
        "# Report the size of each class\n",
        "unique, counts = np.unique(y_train_val_combined, return_counts=True)\n",
        "class_distribution = dict(zip(unique, counts))\n",
        "\n",
        "print(\"Class distribution in training + validation set:\")\n",
        "print(f\"Malignant (0): {class_distribution[0]}\")\n",
        "print(f\"Benign (1): {class_distribution[1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lAOA9nJ1w-Q",
        "outputId": "c297ce0d-ede9-49d2-907a-3dda17a16319"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class distribution in training + validation set:\n",
            "Malignant (0): 169\n",
            "Benign (1): 286\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(d) Train a binary logistic regression model using your implementation from problem 3. Initialize\n",
        "the model weights randomly, sampling from a standard Gaussian distribution. Experiment with\n",
        "different choices of fixed learning rate and batch size."
      ],
      "metadata": {
        "id": "IDIQ2dnN3j6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "learning_rate = 0.01\n",
        "batch_size = 32\n",
        "max_iters = 1000\n",
        "\n",
        "# Train the model\n",
        "trained_weights = train_logistic_regression(X_train, y_train, learning_rate, batch_size, max_iters)\n",
        "\n",
        "trained_weights\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibQ5utlg2Txx",
        "outputId": "9e5d18ff-c45e-46a3-9520-a475ca6cb2f7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 10.26091871,  -9.76076151,  42.17274633,   6.7202822 ,\n",
              "        -1.02326443,  -1.77819733,  -0.48800564,  -2.21783767,\n",
              "         1.13994331,   1.35521215,  -0.36709308,   0.56698365,\n",
              "        -0.78796832, -17.21830295,  -0.18170578,   0.66718687,\n",
              "        -0.41033344,   1.15142781,  -0.59790463,   0.15251699,\n",
              "        10.59269902, -21.31083455,  28.09641288, -12.03489545,\n",
              "        -0.56938357,  -1.97846294,  -2.10656458,  -0.34556878,\n",
              "        -0.76861342,  -0.44749083])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(e) Use the trained model to report the performance of the model on the test set. For evaluation\n",
        "metrics, use accuracy, precision, recall, and F1-score."
      ],
      "metadata": {
        "id": "N0s880xZ3zUK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Predict on the test set\n",
        "y_test_pred = predict(X_test, trained_weights)\n",
        "\n",
        "# Evaluate performance\n",
        "accuracy = accuracy_score(y_test, y_test_pred)\n",
        "precision = precision_score(y_test, y_test_pred)\n",
        "recall = recall_score(y_test, y_test_pred)\n",
        "f1 = f1_score(y_test, y_test_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1-score: {f1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVpUeDH066G4",
        "outputId": "c22f3af8-eb66-4cbe-f372-004881e7ce05"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9473684210526315\n",
            "Precision: 0.922077922077922\n",
            "Recall: 1.0\n",
            "F1-score: 0.9594594594594594\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(f) Summarize your findings."
      ],
      "metadata": {
        "id": "7t_7s7WOx1tn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model correctly classified about 94.7% of the test samples. Of all the instances the model predicted as positive, 92.2% were actually positive. This is a good indication of the model’s ability to avoid false positives. The model identified all actual positive cases correctly.The F1-score balances precision and recall, showing a high score of 95.9%, which indicates the model has a good trade-off between these measures.\n",
        "\n"
      ],
      "metadata": {
        "id": "CUvO1yf44CJM"
      }
    }
  ]
}